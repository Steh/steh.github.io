---
title: "Splunk"
categories: 
  - splunk
tags:
  - blue team
classes: 
  - wide
excerpt: ""
toc: true
date: 2023-10-11
---

## commands

### convert unixtimestamps

```bash
| eval unix_time = 1725573600
| eval datetime=strftime(unix_time, "%Y-%m")

# Examples
%Y-%m-%d    ->  2021-12-31
%y-%m-%d    ->  21-12-31
%b %d, %Y   ->  Feb 11, 2022 
```

* [Using time variables][def1]

### streamstats

used for calculating statistics and adding them as new fields in your search results, based on the order in which the events are encountered.

```bash
# current   - option means the current event's value isn't included when calculating ( f = false, t = true)
# window    - limits to just the next event

| streamstats current=f window=1 last(value) as last_value by cve
```

## sourcetype

### json

```bash
[json]
KV_MODE = json
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+){
```

"If you set KV_MODE = json, do not also set INDEXED_EXTRACTIONS = JSON for the same source type. If you do this, the json fields are extracted twice, once at index time and again at search time."

* [splunk: Configure automatic key-value field extraction][def]

## Environment Variables

```bash
/opt/splunk/bin/splunk envvars
```

## Split Data from one sourcetype into different indexes

* create indexes
* i would recommend to create a new app to gather all config files in one place
* transforms.conf
  * define the transformation
  * Regex to find the data
  * Format defines the Target Index
* props.conf
  * combines the sourcetype with the transformation
  * Transforms-<Key> - defines the transformation that will be used on the data
  * key value can be any values you want, it doesnÂ´t matter

```bash
# input
## inputs.conf
[monitor://path/to/data]
index = <index>
sourcetype = some_data_sourcetype

# transformation
## transforms.conf
[transformation_for_somedata]
REGEX = <matching_criteria>
DEST_KEY = _MetaData:Index
Format = <target-index>

# sourcetype
## props.conf
[some_data_sourcetype]
TRANSFORMS-index = transformation_for_somedata
```

## RBA
### Where do Risk Scores Come From

1. Adaptive Response Actions
2. Manual Risk Score (```| eval risk_score=50```)
3. Risk Factor Editor
 * Can use addition or multiplication to raise Risk Score
 * Multiply by 0 to change Risk Score to 0

```bash
# see all combined
| from datamodel:"Risk"
| table source risk_factor_add risk_factor_mult risk_score
```

### How to weight?

1. MITRE ATT&CK Weight + Use Case Weight
2. Weight by Volume
   * Track how often detections are firing
   * create more depth to risk scores
   * ```| from datamodel:"Risk"."All_Risk"
| search `risk_notable_sources`
| stats count by search_name
| eval avg=round(count/30)
| eval velocity=case(avg<=1,1.25,avg>1 AND
avg<=50,1,avg>50 AND avg<=100,0.75,avg>100 AND
avg<=500,0.5,avg>500,0.25)
| outputlookup risk_velocity.csv```

* [New High Score - How to Play RBA and Win!][def2]
* [Splunk RBA github][def3]
* [mitre_att&ck_weight][def4]

## source

[def]: https://docs.splunk.com/Documentation/Splunk/latest/Knowledge/Automatickey-valuefieldextractionsatsearch-time
[def1]: https://github.com/matt-snyder-stuff/.conf2024
[def2]: https://conf.splunk.com/files/2024/slides/SEC1186C.pdf
[def3]: https://github.com/splunk/rba/
[def4]: https://github.com/matt-snyder-stuff/.conf2024/blob/main/mitre_att%26ck_weight.csv
